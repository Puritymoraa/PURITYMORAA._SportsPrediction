# -*- coding: utf-8 -*-
"""FIFAREGRESSIONASSIGNMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oB8oJwAoBW6NOB-kEz6FGSHOFNDNZS2_
"""



"""**FIFA ASSIGNMENT**
1. Demonstrate the data preparation & feature extraction process.
"""

import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score

# Loading both the training and testing datasets
male_players_data = pd.read_csv('/content/sample_data/male_players (legacy).csv')
players_22_data = pd.read_csv('/content/sample_data/players_22.csv')

male_players_data.head()

players_22_data.head()

all_column_names = list(male_players_data.columns)
print(all_column_names)

male_players_data

columns_needed = ['overall', 'potential', 'value_eur', 'wage_eur', 'age',
                              'height_cm', 'weight_kg', 'international_reputation',
                              'weak_foot', 'skill_moves', 'work_rate', 'body_type',
                              'real_face', 'release_clause_eur', 'team_position',
                              'team_jersey_number', 'loaned_from', 'joined',
                              'contract_valid_until', 'nation_position',
                              'nation_jersey_number', 'pace', 'shooting', 'passing',
                              'dribbling', 'defending', 'physic']

# common relevant columns present in both datasets
relevant_columns = [col for col in columns_needed if col in male_players_data.columns and col in players_22_data.columns]

# Extracting features and target variable from the training data
X = male_players_data[relevant_columns]
y = male_players_data['overall']

#Processing all the quatitative features and their correlation
quantitaive = X.select_dtypes(include = [np.number])
corr_matrix = quantitaive.corr()
corr_matrix['overall'].sort_values(ascending = False)

# Imputing missing values for numeric data
imputer = SimpleImputer(strategy='median')
q_columns = quantitaive.columns
quantitaive_imputed = imputer.fit_transform(quantitaive) # assign to a new variable to avoid overwriting
# Re-fetch column names after imputation in case a column was removed
#q_columns = imputer.get_feature_names_out() # SimpleImputer can tell us the new columns
#quantitative = pd.DataFrame(quantitaive_imputed, columns=q_columns) # Recreate DataFrame
#quantitaive

# Fill missing values with the mean for numerical features
quantitaive.fillna(quantitaive.mean(), inplace=True)

#Scaling the imputed numeric  data
scaler = StandardScaler()
quantitaive = pd.DataFrame(quantitaive, columns=q_columns) # Convert quantitaive back to a DataFrame
quantitaive = pd.DataFrame(scaler.fit_transform(quantitaive), columns = quantitaive.columns)
quantitaive

quantitaive.drop('overall', axis = 1, inplace = True)
import matplotlib.pyplot as plt

# Distribution plots for numeric variables
numeric_columns = quantitaive.columns
for column in numeric_columns:
    plt.figure(figsize=(10, 4))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(quantitaive[column], kde=True)
    plt.title(f'Distribution of {column}')

#Processing all the categorical features and their correlation
categorical = X.select_dtypes(include=['object'])
categorical.info()

categorical.drop(columns = [column for column in categorical.columns if column not in columns_needed], inplace =True)
categorical.info()

categorical = categorical.fillna(categorical.mode().iloc[0])

print(categorical.info())

#Encoding the categorical data using onehot encoder
categorical = pd.get_dummies(categorical, drop_first=True)
categorical

#replacing NaN with mode
categorical.fillna(categorical.mode().iloc[0], inplace=True)

#concatinating the categorical and numeric data
X = pd.concat([categorical, quantitaive], axis = 1)

#standardizing the feartures of X
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)
X

#Now, we will split the training data to training and validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

#We will use four models
models = {
    'Linear Regression': LinearRegression(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'XGBoost': XGBRegressor()
}

#Evaluating the models using cross-validation
results = {}
for model_name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    results[model_name]= np.sqrt(-scores)

print("Cross-validation results:")
for model_name, scores in results.items():
    print(f"{model_name}: {scores.mean():.2f} (+/- {scores.std():.2f}))")

"""Since XGBoost regresor has the lowest RMSE, we are going to use RMSE and then finetune it, train and test it again."""

# fine tuning the model, training then testing them again:
# Hyperparameter grids for each model
param_grid = {
        'n_estimators': [100, 200, 300, 400],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    }

# Initialize the GridSearchCV
grid_search = GridSearchCV(XGBRegressor(objective='reg:squarederror'), param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

# Fit GridSearchCV on the training data
grid_search.fit(X_train, y_train)

# Get the best estimator
best_xgb_model = grid_search.best_estimator_

print(f"Best parameters: {grid_search.best_params_}")

# Train the best model on the full training data
best_xgb_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = best_xgb_model.predict(X_val)

# Evaluate the model using MAE and RMSE
mse = mean_squared_error(y_val, y_val_pred)
rmse = np.sqrt(mse)

print(f"""Fine-Tuned XGBoost Regressor:
    Root Mean Squared Error = {rmse}
""")

"""**Use the data(players_22) to test how good is the model with completely new data**"""

#Getting the data ready for testing
X_test = players_22_data[relevant_columns]
y_test = players_22_data['overall']

quant= X_test.select_dtypes(include = [np.number])
imputer = SimpleImputer(strategy='median')
qcolumns = quant.columns
quant_imputed = imputer.fit_transform(quant)
quant.fillna(quant.mean(), inplace=True)

scaler = StandardScaler()
quant = pd.DataFrame(quant, columns=qcolumns) # Convert quantitaive back to a DataFrame
quant = pd.DataFrame(scaler.fit_transform(quant), columns = quant.columns)
quant.drop('overall', axis = 1, inplace = True)

#Processing all the categorical features and their correlation
cat = X_test.select_dtypes(include=['object'])
cat.drop(columns = [column for column in cat.columns if column not in columns_needed], inplace =True)
cat = cat.fillna(cat.mode().iloc[0])
#Encoding the categorical data using onehot encoder
cat = pd.get_dummies(cat, drop_first=True)
#replacing NaN with mode
cat.fillna(cat.mode().iloc[0], inplace=True)

#concatinating the categorical and numeric data from the testing dataset
X_test = pd.concat([cat, quant], axis = 1)

#standardizing the feartures of X_test
scaler = StandardScaler()
X_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)
X_test

# Predic the overall ratings for the test data
y_test_pred = players_22_data['overall']
y_test_pred= best_xgb_model.predict(X_test)

y_test_pred

# Evaluate the model using MAE and RMSE
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
mae_players_22 = mean_absolute_error(y_test, y_test_pred)
mse_players_22 = mean_squared_error(y_test, y_test_pred)
rmse_players_22 = np.sqrt(mse_players_22)
r2_players_22 = r2_score(y_test, y_test_pred)

print(f"""Performance on players_22 dataset:
    Mean Absolute Error = {mae_players_22}
    Mean Squared Error = {mse_players_22}
    Root Mean Squared Error = {rmse_players_22}
    R2 score = {r2_players_22}
""")

"""**Saving the predicted data in a csv file**"""

# Save the predictions
import pickle
players_22_data.to_pickle('predicted_players_22.pkl')
print("Predictions saved to predicted_players_22.pkl")

filename = 'predicted_players_22.pkl'
pickle.dump(filename, open(filename, 'wb'))
saved_file = pickle.load(open(filename, 'rb'))

# Save the model
loaded_model = 'best_xgb_model.pkl'
pickle.dump(best_xgb_model, open(loaded_model, 'wb'))
loaded_model = pickle.load(open(loaded_model, 'rb'))

